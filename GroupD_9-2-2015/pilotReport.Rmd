---
title: "Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---

[TEXT IN SQUARE BRACKETS IS HERE FOR GUIDANCE. PLEASE DELETE TEXT IN SQUARE BRACKETS BEFORE KNITTING THE FINAL REPORT]

# Report Details

[PILOT/COPILOT ENTER RELEVANT REPORT DETAILS HERE]

```{r}
articleID <- "9-2-2015" # insert the article ID code here e.g., "10-3-2015"
reportType <- 'pilot' # specify whether this is the 'pilot' report or 'copilot' report
pilotNames <- "Nicky Sullivan" # insert the pilot's name here e.g., "Tom Hardwicke".
copilotNames <- NA # # insert the co-pilot's name here e.g., "Michael Frank".
pilotTTC <- NA # insert the pilot's estimated time to complete (in minutes, it is fine to approximate) e.g., 120
copilotTTC <- NA # insert the co-pilot's estimated time to complete (in minutes, it is fine to approximate) e.g., 120
pilotStartDate <- as.Date("11/03/2019") # insert the piloting start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
copilotStartDate <- NA # insert the co-piloting start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
completionDate <- NA # insert the date of final report completion in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
```

------

#### Methods summary: 

[PILOT/COPILOT write a brief summary of the methods underlying the target outcomes written in your own words]
Each participant completes 160 trials, the first 155 of which are identical and involve a fixation screen, then a brief (150ms) display of the stimulus, which consisted of four items, one in each corner of the screen. The stimulus consisted of three numbers and one letter, each in a different color (red, blue, yellow, or magenta). After seeing the stimulus, participants are then asked to report which corner the letter was in by clicking a corresponding number (1 through 4). On the last 5 trials, participants see the same type of stimulus, but instead of being asked to report where the letter was, they are asked to report what letter was shown and what color it was (order counterbalanced between participants). The first of these trials (#156 overall) is considered the 'surprise' trial, and the remaining four are considered 'control' trials

------

#### Target outcomes: 

[PILOT copy and paste the exact target outcomes as written in in targetOutcomes.md]  
For this article you should focus on the findings reported in results section of Experiment 1a.

Specifically, you should attempt to reproduce all descriptive and inferential analyses reported in the text below and associated tables/figures:

>On the presurprise trials, 89% of responses in the location task were correct, which indicates that participants could easily locate the target by using the critical attribute. To analyze the data from the surprise trial, we first divided participants into two groups defined by the order of the surprise tasks (identity task first vs. color task first). We found that the results were almost the same in these two groups. Accordingly, we combined the data for these groups in the analyses reported here. Only 6 of 20 (30%) participants correctly reported the color of the target letter, which is not much better than chance level of 25% (because there were four choices). Furthermore, performance on the identity task (25% correct) was exactly at chance level. These results demonstrate that participants were not capable of reporting a task-relevant attribute of a stimulus that had reached awareness less than 1 s before (i.e., attribute amnesia). Moreover, in the surprise trial, participants’ performance on the location task, unlike their performance on the color and identity tasks, was good (80% correct), and in fact was approximately as good as their performance on the location task in the presurprise trials (89% correct). This indicates that the poor performance on the color and identity tasks was not induced by the surprise test itself; it more likely reflects participants’ failure to remember these attributes. Participants exhibited a dramatic increase in reporting accuracy for the target letter’s color (70% correct) and identity (75% correct) on the first control trial (i.e., the trial immediately after the surprise trial). The improvement in each case was significant—color: 70% versus 30%, χ2(1, N = 40) = 6.40, p = .011, ϕ = .40; identity: 75% versus 25%, χ2(1, N = 40) = 10.00, p < .005, ϕ = .50. Performance on these two tasks remained constant on the final three control trials (color: 75%, 70%, and 80% correct; identity: 75%, 80%, and 75% correct). Participants’ performance on the location task was almost the same on the surprise trial (80% correct) as on the control trials (80%, 85%, 80%, and 70% correct). These results indicate a crucial role for expectation in controlling participants’ ability to report the attributes of a consciously perceived object. Therefore, Experiment 1a showed that when participants did not expect to report a particular attribute of an attended object, they were incapable of doing so, even when that same attribute had reached awareness immediately prior to the test.
------

[PILOT/COPILOT DO NOT CHANGE THE CODE IN THE CHUNK BELOW]  

```{r global_options, include=FALSE}
# sets up some formatting options for the R Markdown document
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

# Step 1: Load packages and prepare report object

[PILOT/COPILOT Some useful packages are being loaded below. You can add any additional ones you might need too.]

```{r}
# load packages
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(ReproReports) # custom reporting functions
```

[PILOT/COPILOT DO NOT MAKE CHANGES TO THE CODE CHUNK BELOW]

```{r}
# Prepare report object. This will be updated automatically by the reproCheck function each time values are compared
reportObject <- data.frame(dummyRow = TRUE, reportedValue = NA, obtainedValue = NA, valueType = NA, percentageError = NA, comparisonOutcome = NA, eyeballCheck = NA)
```

# Step 2: Load data

This will set the working directory and then read in the data
```{r}
setwd("/Users/nickysullivan/Documents/Github/reproducibility_assignment/GroupD_9-2-2015/data/materials-9859-Top-level_materials") 
data_reproduce <- read.csv("12022-Exp1.csv") 
```

# Step 3: Tidy data

This section wil rename all of the columns to be more descriptive (based on the MetaData document included with the data)

```{r}
data_reproduce_tidy <- data_reproduce %>%
  rename(subid = X6, trial = X1.1, target_color = X1.2, target_identity = X3, target_location = X3.1, color_response = X0, identity_response = X0.1, location_response = X1.3, color_accuracy = X0.2, identity_accuracy = X0.3, location_accuracy = X0.4)

```

# Step 4: Run analysis

## Pre-processing

The authors report that there are no exclusions, so no pre-processing should be necessary

```{r}
```

## Descriptive statistics

The first finding we'll try and reproduce is the average accuracy on location questions across all pre-surprise trials
> On the presurprise trials, 89% of responses in the location task were correct

```{r}
ps_accuracy <- data_reproduce_tidy %>%
  filter(trial < 156) %>%
  summarise(acc = mean(location_accuracy))
ps_accuracy
```

The second finding we'll try and reproduce is the average accuracies on the color, identity, and location questions in the surprise trial

```{r}
sur_accuracy <- data_reproduce_tidy %>%
  filter(trial == 156) %>%
  summarise(count = n(),
            color_acc = mean(color_accuracy), 
            iden_acc = mean(identity_accuracy),
            loc_acc = mean(location_accuracy))
sur_accuracy
```

Next we'll look at the performance on color, identity, and location questions on the control trials

```{r}
control_accuracy <- data_reproduce_tidy %>%
  filter(trial > 156) %>%
  group_by(trial) %>%
  summarise(count = n(),
            color_acc = mean(color_accuracy),
            iden_acc = mean(identity_accuracy),
            loc_acc = mean(location_accuracy))
control_accuracy
```


## Inferential statistics

The first inferential test we want to reproduce is a chi square test comparing performance on the color question between the surprise trial and the first control trial

```{r}
control_comp <-data_reproduce_tidy %>%
  filter(trial == 156 | trial == 157)
chisq.test()
```

# Step 5: Conclusion

[Please include a text summary describing your findings. If this reproducibility check was a failure, you should note any suggestions as to what you think the likely cause(s) might be.]


[PILOT/COPILOT DOD NOT EDIT THE CODE CHUNK BELOW]

```{r}
reportObject <- reportObject %>%
  filter(dummyRow == FALSE) %>% # remove the dummy row
  select(-dummyRow) %>% # remove dummy row designation
  mutate(articleID = articleID) %>% # add variables to report 
  select(articleID, everything()) # make articleID first column

# decide on final outcome
if(any(reportObject$comparisonOutcome %in% c("MAJOR_ERROR", "DECISION_ERROR"))){
  finalOutcome <- "Failure"
}else{
  finalOutcome <- "Success"
}

# collate report extra details
reportExtras <- data.frame(articleID, pilotNames, copilotNames, pilotTTC, copilotTTC, pilotStartDate, copilotStartDate, completionDate, finalOutcome)

# save report objects
if(reportType == "pilot"){
  write_csv(reportObject, "pilotReportDetailed.csv")
  write_csv(reportExtras, "pilotReportExtras.csv")
}

if(reportType == "copilot"){
  write_csv(reportObject, "copilotReportDetailed.csv")
  write_csv(reportExtras, "copilotReportExtras.csv")
}
```

# Session information

[This function will output information about the package versions used in this report:]

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
